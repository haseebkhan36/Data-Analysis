{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASBD tutorial 8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  For the given input file, calculate Wordcount using Hadop and Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With spark RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('What', 1),\n",
       " ('is', 8),\n",
       " ('Lorem', 17),\n",
       " ('', 7),\n",
       " ('Ipsum', 13),\n",
       " ('dummy', 2),\n",
       " ('of', 21),\n",
       " ('printing', 1),\n",
       " ('typesetting', 1),\n",
       " (\"industry's\", 1),\n",
       " ('ever', 1),\n",
       " ('1500s,', 1),\n",
       " ('when', 2),\n",
       " ('an', 1),\n",
       " ('unknown', 1),\n",
       " ('took', 1),\n",
       " ('galley', 1),\n",
       " ('type', 2),\n",
       " ('make', 1),\n",
       " ('book.', 1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "import timeit\n",
    "sc = spark.sparkContext\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "words = sc.textFile('sample.txt').flatMap(lambda line: line.split(\" \"))\n",
    "wordCounts = words.map(lambda word: (word, 1)).reduceByKey(lambda a,b : a+b)\n",
    "end_time = timeit.default_timer()\n",
    "\n",
    "time_taken_RDD = end_time - start_time\n",
    "\n",
    "wordCounts.take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Spark RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what : 1\n",
      "is : 8\n",
      "lorem : 17\n",
      "ipsum? : 1\n",
      " : 7\n",
      "ipsum : 14\n",
      "simply : 2\n",
      "dummy : 2\n",
      "text : 2\n",
      "of : 21\n",
      "the : 27\n",
      "printing : 1\n",
      "and : 10\n",
      "typesetting : 1\n",
      "industry. : 1\n",
      "has : 4\n",
      "been : 1\n",
      "industry's : 1\n",
      "standard : 2\n",
      "ever : 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import timeit\n",
    "count = 0\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "text = open(\"sample.txt\", \"r\")\n",
    "d = dict()\n",
    "for line in text:\n",
    "    line = line.strip()\n",
    "    line = line.lower()\n",
    "    words = line.split(\" \")\n",
    "    for word in words:\n",
    "        if word in d:\n",
    "            d[word] = d[word] + 1\n",
    "        else:\n",
    "            d[word] = 1\n",
    "\n",
    "end_time = timeit.default_timer()\n",
    "\n",
    "time_taken_NoRDD = end_time - start_time\n",
    "\n",
    "i = 1\n",
    "for key in list(d.keys()):\n",
    "    if(i>20):\n",
    "        break\n",
    "    i += 1\n",
    "    print(key, \":\", d[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken to count words with RDD:  0.055\n",
      "Time Taken to count words without RDD:  0.003\n"
     ]
    }
   ],
   "source": [
    "print(\"Time Taken to count words with RDD: \", round(time_taken_RDD, 3))\n",
    "print(\"Time Taken to count words without RDD: \", round(time_taken_NoRDD, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Populate 1000 numbers , calculate Mean, Variance, Std Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = spark.createDataFrame(pd.DataFrame(np.random.randint(0,100,size=(1000,1)), columns=[\"Numbers\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|Numbers|\n",
      "+-------+\n",
      "|     85|\n",
      "|     87|\n",
      "|     25|\n",
      "|      2|\n",
      "|     70|\n",
      "|     61|\n",
      "|     16|\n",
      "|     66|\n",
      "|     80|\n",
      "|     17|\n",
      "|     32|\n",
      "|      0|\n",
      "|     93|\n",
      "|     40|\n",
      "|     25|\n",
      "|     58|\n",
      "|     62|\n",
      "|     86|\n",
      "|     98|\n",
      "|     28|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|avg(Numbers)|\n",
      "+------------+\n",
      "|      48.759|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean, variance, stddev_samp\n",
    "df.select(mean(df.Numbers)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|var_samp(Numbers)|\n",
      "+-----------------+\n",
      "|842.1030220220222|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(variance(df.Numbers)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|stddev_samp(Numbers)|\n",
      "+--------------------+\n",
      "|  29.019011389467117|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(stddev_samp(df.Numbers)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Compute correlation b/w two series using Pearson’s & Spearman’s method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "data = [[Vectors.dense([35, 23, 47, 17, 10, 43, 9, 6, 28])], [Vectors.dense([30, 33, 45, 23, 8, 49, 12, 4, 31])]]\n",
    "data = spark.createDataFrame(data, [\"A\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                   A|\n",
      "+--------------------+\n",
      "|[35.0,23.0,47.0,1...|\n",
      "|[30.0,33.0,45.0,2...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseMatrix([[ 1., -1.,  1., -1.,  1., -1., -1.,  1., -1.],\n",
      "             [-1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1.],\n",
      "             [ 1., -1.,  1., -1.,  1., -1., -1.,  1., -1.],\n",
      "             [-1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1.],\n",
      "             [ 1., -1.,  1., -1.,  1., -1., -1.,  1., -1.],\n",
      "             [-1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1.],\n",
      "             [-1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1.],\n",
      "             [ 1., -1.,  1., -1.,  1., -1., -1.,  1., -1.],\n",
      "             [-1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "pearsonCorr = Correlation.corr(data, 'A', 'pearson').collect()[0][0]\n",
    "print(str(pearsonCorr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseMatrix([[ 1., -1.,  1., -1.,  1., -1., -1.,  1., -1.],\n",
      "             [-1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1.],\n",
      "             [ 1., -1.,  1., -1.,  1., -1., -1.,  1., -1.],\n",
      "             [-1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1.],\n",
      "             [ 1., -1.,  1., -1.,  1., -1., -1.,  1., -1.],\n",
      "             [-1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1.],\n",
      "             [-1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1.],\n",
      "             [ 1., -1.,  1., -1.,  1., -1., -1.,  1., -1.],\n",
      "             [-1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "spearmanCorr = Correlation.corr(data, 'A', 'spearman').collect()[0][0]\n",
    "print(str(spearmanCorr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Applying e^x and log(x) on 10k numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df10k = spark.createDataFrame(pd.DataFrame(np.random.randint(0,100,size=(10000,1)), columns=[\"Numbers\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### DataFrame for 1000 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|Numbers|\n",
      "+-------+\n",
      "|     80|\n",
      "|     11|\n",
      "|     52|\n",
      "|     82|\n",
      "|     35|\n",
      "|     37|\n",
      "|     20|\n",
      "|     30|\n",
      "|      1|\n",
      "|     85|\n",
      "|     59|\n",
      "|      6|\n",
      "|     99|\n",
      "|     27|\n",
      "|     25|\n",
      "|     41|\n",
      "|     39|\n",
      "|     93|\n",
      "|      2|\n",
      "|     65|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df10k.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df10k.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        EXP(Numbers)|\n",
      "+--------------------+\n",
      "| 5.54062238439351E34|\n",
      "|   59874.14171519782|\n",
      "|3.831008000716577E22|\n",
      "|4.093996962127454...|\n",
      "|1.586013452313430...|\n",
      "|1.171914237280261...|\n",
      "| 4.851651954097903E8|\n",
      "|1.068647458152446...|\n",
      "|  2.7182818284590455|\n",
      "|8.223012714622913E36|\n",
      "|4.201210403790514...|\n",
      "|   403.4287934927351|\n",
      "|9.889030319346946E42|\n",
      "|5.320482406017986...|\n",
      "|7.200489933738588E10|\n",
      "|6.398434935300549...|\n",
      "|8.659340042399374...|\n",
      "|2.451245542920086E40|\n",
      "|    7.38905609893065|\n",
      "|1.694889244410333...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import exp\n",
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "df10k.select(exp(df10k.Numbers)).show()\n",
    "end_time = timeit.default_timer()\n",
    "\n",
    "time_taken = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken for 10k entries:  0.037\n"
     ]
    }
   ],
   "source": [
    "print(\"Time Taken for 10k entries: \", round(time_taken, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|       ln(Numbers)|\n",
      "+------------------+\n",
      "| 4.382026634673881|\n",
      "|2.3978952727983707|\n",
      "|3.9512437185814275|\n",
      "| 4.406719247264253|\n",
      "|3.5553480614894135|\n",
      "|3.6109179126442243|\n",
      "| 2.995732273553991|\n",
      "|3.4011973816621555|\n",
      "|               0.0|\n",
      "| 4.442651256490317|\n",
      "|  4.07753744390572|\n",
      "| 1.791759469228055|\n",
      "|  4.59511985013459|\n",
      "| 3.295836866004329|\n",
      "|3.2188758248682006|\n",
      "| 3.713572066704308|\n",
      "|3.6635616461296463|\n",
      "| 4.532599493153256|\n",
      "|0.6931471805599453|\n",
      "| 4.174387269895637|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import log\n",
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "df10k.select(log(df10k.Numbers)).show()\n",
    "end_time = timeit.default_timer()\n",
    "\n",
    "time_taken = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken for 10k entries:  0.048\n"
     ]
    }
   ],
   "source": [
    "print(\"Time Taken for 10k entries: \", round(time_taken, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DataFrame For 100 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df100 = df10k.sample(fraction=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|Numbers|\n",
      "+-------+\n",
      "|     11|\n",
      "|     62|\n",
      "|      6|\n",
      "|     24|\n",
      "|     95|\n",
      "|     37|\n",
      "|     33|\n",
      "|     21|\n",
      "|     84|\n",
      "|     41|\n",
      "|     53|\n",
      "|     98|\n",
      "|     41|\n",
      "|     93|\n",
      "|      9|\n",
      "|     12|\n",
      "|     96|\n",
      "|     19|\n",
      "|     79|\n",
      "|     24|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df100.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df100.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        EXP(Numbers)|\n",
      "+--------------------+\n",
      "|   59874.14171519782|\n",
      "|8.438356668741455E26|\n",
      "|   403.4287934927351|\n",
      "|2.648912212984347E10|\n",
      "|1.811239082889023...|\n",
      "|1.171914237280261...|\n",
      "|2.146435797859160...|\n",
      "|1.3188157344832146E9|\n",
      "|3.025077322201142...|\n",
      "|6.398434935300549...|\n",
      "|1.041375943302908...|\n",
      "|3.637970947608805E42|\n",
      "|6.398434935300549...|\n",
      "|2.451245542920086E40|\n",
      "|   8103.083927575384|\n",
      "|  162754.79141900392|\n",
      "|4.923458286012058E41|\n",
      "|1.7848230096318728E8|\n",
      "|2.038281066512668...|\n",
      "|2.648912212984347E10|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import exp\n",
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "df100.select(exp(df100.Numbers)).show()\n",
    "end_time = timeit.default_timer()\n",
    "\n",
    "time_taken = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken for 100 entries:  0.081\n"
     ]
    }
   ],
   "source": [
    "print(\"Time Taken for 100 entries: \", round(time_taken, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|       ln(Numbers)|\n",
      "+------------------+\n",
      "|2.3978952727983707|\n",
      "| 4.127134385045092|\n",
      "| 1.791759469228055|\n",
      "|3.1780538303479458|\n",
      "| 4.553876891600541|\n",
      "|3.6109179126442243|\n",
      "|3.4965075614664802|\n",
      "| 3.044522437723423|\n",
      "| 4.430816798843313|\n",
      "| 3.713572066704308|\n",
      "| 3.970291913552122|\n",
      "| 4.584967478670572|\n",
      "| 3.713572066704308|\n",
      "| 4.532599493153256|\n",
      "|2.1972245773362196|\n",
      "|2.4849066497880004|\n",
      "| 4.564348191467836|\n",
      "|2.9444389791664403|\n",
      "|4.3694478524670215|\n",
      "|3.1780538303479458|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import log\n",
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "df100.select(log(df100.Numbers)).show()\n",
    "end_time = timeit.default_timer()\n",
    "\n",
    "time_taken = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken for 100 entries:  0.084\n"
     ]
    }
   ],
   "source": [
    "print(\"Time Taken for 100 entries: \", round(time_taken, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 5. Generate FIM using FP Growth on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------+\n",
      "|items                                                                                         |\n",
      "+----------------------------------------------------------------------------------------------+\n",
      "|[1, 3, 9, 13, 23, 25, 34, 36, 38, 40, 52, 54, 59, 63, 67, 76, 85, 86, 90, 93, 98, 107, 113, ] |\n",
      "|[2, 3, 9, 14, 23, 26, 34, 36, 39, 40, 52, 55, 59, 63, 67, 76, 85, 86, 90, 93, 99, 108, 114, ] |\n",
      "|[2, 4, 9, 15, 23, 27, 34, 36, 39, 41, 52, 55, 59, 63, 67, 76, 85, 86, 90, 93, 99, 108, 115, ] |\n",
      "|[1, 3, 10, 15, 23, 25, 34, 36, 38, 41, 52, 54, 59, 63, 67, 76, 85, 86, 90, 93, 98, 107, 113, ]|\n",
      "|[2, 3, 9, 16, 24, 28, 34, 37, 39, 40, 53, 54, 59, 63, 67, 76, 85, 86, 90, 94, 99, 109, 114, ] |\n",
      "|[2, 3, 10, 14, 23, 26, 34, 36, 39, 41, 52, 55, 59, 63, 67, 76, 85, 86, 90, 93, 98, 108, 114, ]|\n",
      "|[2, 4, 9, 15, 23, 26, 34, 36, 39, 42, 52, 55, 59, 63, 67, 76, 85, 86, 90, 93, 98, 108, 115, ] |\n",
      "|[2, 4, 10, 15, 23, 27, 34, 36, 39, 41, 52, 55, 59, 63, 67, 76, 85, 86, 90, 93, 99, 107, 115, ]|\n",
      "|[1, 3, 10, 15, 23, 25, 34, 36, 38, 43, 52, 54, 59, 63, 67, 76, 85, 86, 90, 93, 98, 110, 114, ]|\n",
      "|[2, 4, 9, 14, 23, 26, 34, 36, 39, 42, 52, 55, 59, 63, 67, 76, 85, 86, 90, 93, 98, 107, 115, ] |\n",
      "|[2, 3, 10, 14, 23, 27, 34, 36, 39, 42, 52, 55, 59, 63, 67, 76, 85, 86, 90, 93, 99, 108, 114, ]|\n",
      "|[2, 3, 10, 14, 23, 26, 34, 36, 39, 41, 52, 55, 59, 63, 67, 76, 85, 86, 90, 93, 98, 107, 115, ]|\n",
      "|[2, 4, 9, 14, 23, 26, 34, 36, 39, 44, 52, 55, 59, 63, 67, 76, 85, 86, 90, 93, 99, 107, 114, ] |\n",
      "|[1, 3, 10, 15, 23, 25, 34, 36, 38, 40, 52, 54, 59, 63, 67, 76, 85, 86, 90, 93, 99, 110, 113, ]|\n",
      "|[2, 3, 11, 13, 24, 28, 34, 37, 39, 41, 53, 54, 59, 64, 67, 76, 85, 86, 90, 94, 98, 109, 114, ]|\n",
      "|[2, 5, 11, 16, 24, 28, 34, 36, 38, 40, 52, 54, 59, 63, 67, 76, 85, 86, 90, 93, 99, 111, 113, ]|\n",
      "|[2, 6, 11, 15, 24, 28, 34, 37, 39, 40, 53, 54, 59, 63, 67, 76, 85, 86, 90, 94, 99, 109, 114, ]|\n",
      "|[1, 3, 9, 13, 23, 25, 34, 36, 38, 41, 52, 54, 59, 63, 67, 76, 85, 86, 90, 93, 98, 107, 114, ] |\n",
      "|[1, 3, 10, 15, 23, 25, 34, 36, 38, 41, 52, 54, 59, 63, 67, 76, 85, 86, 90, 93, 99, 107, 113, ]|\n",
      "|[1, 3, 9, 13, 23, 25, 34, 36, 38, 40, 52, 54, 59, 63, 67, 76, 85, 86, 90, 93, 99, 107, 113, ] |\n",
      "+----------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "data = (spark.read.text(\"mushroom.txt\").select(split(\"value\", \"\\s+\").alias(\"items\")))\n",
    "data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+\n",
      "|items         |freq|\n",
      "+--------------+----+\n",
      "|[86]          |7924|\n",
      "|[86, ]        |7924|\n",
      "|[86, , 85]    |7924|\n",
      "|[86, 85]      |7924|\n",
      "|[90]          |7488|\n",
      "|[90, ]        |7488|\n",
      "|[90, , 85]    |7488|\n",
      "|[90, 85]      |7488|\n",
      "|[]            |8124|\n",
      "|[, 85]        |8124|\n",
      "|[34]          |7914|\n",
      "|[34, 86]      |7906|\n",
      "|[34, 86, ]    |7906|\n",
      "|[34, 86, , 85]|7906|\n",
      "|[34, 86, 85]  |7906|\n",
      "|[34, ]        |7914|\n",
      "|[34, , 85]    |7914|\n",
      "|[34, 85]      |7914|\n",
      "|[85]          |8124|\n",
      "+--------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.fpm import FPGrowth\n",
    "fp = FPGrowth(minSupport=0.9, minConfidence=0.7)\n",
    "fpm = fp.fit(data)\n",
    "fpm.freqItemsets.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+------------------+------------------+------------------+\n",
      "|antecedent  |consequent|confidence        |lift              |support           |\n",
      "+------------+----------+------------------+------------------+------------------+\n",
      "|[34, 85]    |[86]      |0.9989891331817033|1.0242033970176878|0.9731659281142294|\n",
      "|[34, 85]    |[]        |1.0               |1.0               |0.9741506646971935|\n",
      "|[34, , 85]  |[86]      |0.9989891331817033|1.0242033970176878|0.9731659281142294|\n",
      "|[85]        |[86]      |0.9753815854258986|1.0               |0.9753815854258986|\n",
      "|[85]        |[90]      |0.9217134416543574|1.0               |0.9217134416543574|\n",
      "|[85]        |[]        |1.0               |1.0               |1.0               |\n",
      "|[85]        |[34]      |0.9741506646971935|1.0               |0.9741506646971935|\n",
      "|[34, 86, 85]|[]        |1.0               |1.0               |0.9731659281142294|\n",
      "|[34, 86, ]  |[85]      |1.0               |1.0               |0.9731659281142294|\n",
      "|[34]        |[86]      |0.9989891331817033|1.0242033970176878|0.9731659281142294|\n",
      "|[34]        |[]        |1.0               |1.0               |0.9741506646971935|\n",
      "|[34]        |[85]      |1.0               |1.0               |0.9741506646971935|\n",
      "|[90, ]      |[85]      |1.0               |1.0               |0.9217134416543574|\n",
      "|[34, 86]    |[]        |1.0               |1.0               |0.9731659281142294|\n",
      "|[34, 86]    |[85]      |1.0               |1.0               |0.9731659281142294|\n",
      "|[86, , 85]  |[34]      |0.9977284199899041|1.0242033970176878|0.9731659281142294|\n",
      "|[, 85]      |[86]      |0.9753815854258986|1.0               |0.9753815854258986|\n",
      "|[, 85]      |[90]      |0.9217134416543574|1.0               |0.9217134416543574|\n",
      "|[, 85]      |[34]      |0.9741506646971935|1.0               |0.9741506646971935|\n",
      "|[86, 85]    |[]        |1.0               |1.0               |0.9753815854258986|\n",
      "+------------+----------+------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fpm.associationRules.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
